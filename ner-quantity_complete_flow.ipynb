{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-25T06:46:10.107361Z",
     "iopub.status.busy": "2024-11-25T06:46:10.106999Z",
     "iopub.status.idle": "2024-11-25T06:46:10.458224Z",
     "shell.execute_reply": "2024-11-25T06:46:10.457196Z",
     "shell.execute_reply.started": "2024-11-25T06:46:10.107328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/config.json\n",
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/spiece.model\n",
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/tokenizer.json\n",
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/tokenizer_config.json\n",
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/model.safetensors\n",
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/special_tokens_map.json\n",
      "/kaggle/input/btp_t5_4epoch/transformers/default/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:46:10.946665Z",
     "iopub.status.busy": "2024-11-25T06:46:10.946187Z",
     "iopub.status.idle": "2024-11-25T06:46:22.611925Z",
     "shell.execute_reply": "2024-11-25T06:46:22.610976Z",
     "shell.execute_reply.started": "2024-11-25T06:46:10.946634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gliner\n",
      "  Downloading gliner-0.2.13-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from gliner) (2.4.0)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /opt/conda/lib/python3.10/site-packages (from gliner) (4.45.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.4 in /opt/conda/lib/python3.10/site-packages (from gliner) (0.25.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gliner) (4.66.4)\n",
      "Collecting onnxruntime (from gliner)\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from gliner) (0.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.4->gliner) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.4->gliner) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.4->gliner) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.4->gliner) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.4->gliner) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.4->gliner) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->gliner) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->gliner) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->gliner) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->gliner) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->gliner) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->gliner) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->gliner) (0.20.0)\n",
      "Collecting coloredlogs (from onnxruntime->gliner)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime->gliner) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime->gliner) (3.20.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.21.4->gliner) (3.1.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->gliner) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.0->gliner) (1.3.0)\n",
      "Downloading gliner-0.2.13-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, gliner\n",
      "Successfully installed coloredlogs-15.0.1 gliner-0.2.13 humanfriendly-10.0 onnxruntime-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gliner -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:46:22.614002Z",
     "iopub.status.busy": "2024-11-25T06:46:22.613691Z",
     "iopub.status.idle": "2024-11-25T06:46:22.618666Z",
     "shell.execute_reply": "2024-11-25T06:46:22.617813Z",
     "shell.execute_reply.started": "2024-11-25T06:46:22.613975Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:46:24.935859Z",
     "iopub.status.busy": "2024-11-25T06:46:24.935022Z",
     "iopub.status.idle": "2024-11-25T06:46:33.726651Z",
     "shell.execute_reply": "2024-11-25T06:46:33.725752Z",
     "shell.execute_reply.started": "2024-11-25T06:46:24.935826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:46:33.728766Z",
     "iopub.status.busy": "2024-11-25T06:46:33.728460Z",
     "iopub.status.idle": "2024-11-25T06:47:50.123513Z",
     "shell.execute_reply": "2024-11-25T06:47:50.122799Z",
     "shell.execute_reply.started": "2024-11-25T06:46:33.728738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97cb1312b244a20b65dd2d24e301607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339a0535383646adb042beef78054ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_florence2.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large-ft:\n",
      "- configuration_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e36c1acf9b4ee2a1d9277c6b76bfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_florence2.py:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large-ft:\n",
      "- modeling_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e021b855d4c7a83d5cbb1ef2bca18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efa1a28dc85402286b096eda45817a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c765929b3653474eb751b66a9e8e21c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba185546c2c44f81a9e0bac32fa29bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_florence2.py:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-large-ft:\n",
      "- processing_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b7dd329ed64e308595ddda3082c3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be3d82e63be407b870dc2d65e451019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704b8fb2abd6446286b8ed36154d25e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large-ft\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:47:50.124844Z",
     "iopub.status.busy": "2024-11-25T06:47:50.124562Z",
     "iopub.status.idle": "2024-11-25T06:47:50.132708Z",
     "shell.execute_reply": "2024-11-25T06:47:50.131688Z",
     "shell.execute_reply.started": "2024-11-25T06:47:50.124816Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "def extract_text_from_image(image_source):\n",
    "    try:\n",
    "\n",
    "        if image_source.startswith(\"http://\") or image_source.startswith(\"https://\"):\n",
    "            response = requests.get(image_source)\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "        else:\n",
    "            image = Image.open(image_source)\n",
    "        \n",
    "        prompt = \"<OCR>\"\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=524,\n",
    "            num_beams=3,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        parsed_answer = processor.post_process_generation(generated_text, task=\"<OCR>\", image_size=(image.width, image.height))\n",
    "\n",
    "        return parsed_answer\n",
    "\n",
    "    except UnidentifiedImageError:\n",
    "        return \"Error loading image\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"Error retrieving image from URL\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {e}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:27.841033Z",
     "iopub.status.busy": "2024-11-25T07:05:27.840098Z",
     "iopub.status.idle": "2024-11-25T07:05:29.209211Z",
     "shell.execute_reply": "2024-11-25T07:05:29.208310Z",
     "shell.execute_reply.started": "2024-11-25T07:05:27.840999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: {'<OCR>': '400 LBSpressure < 400 lbsBackrestArm Length 14.6\"Height29.9\"Seat Depth 19.9\".Height44.5\" -47.4\"4.7\" seatSeat Width 23.6\\'Height19.3\" -22.2\"Depth 32.5\\'Width 28.3\\''}\n"
     ]
    }
   ],
   "source": [
    "image_path = \"https://m.media-amazon.com/images/I/41hHTaag7+L.jpg\" \n",
    "text_data = extract_text_from_image(image_path)\n",
    "print(\"Extracted Text:\", text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:38.265795Z",
     "iopub.status.busy": "2024-11-25T07:05:38.264980Z",
     "iopub.status.idle": "2024-11-25T07:05:38.271173Z",
     "shell.execute_reply": "2024-11-25T07:05:38.270255Z",
     "shell.execute_reply.started": "2024-11-25T07:05:38.265759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OCR>': '400 LBSpressure < 400 lbsBackrestArm Length 14.6\"Height29.9\"Seat Depth 19.9\".Height44.5\" -47.4\"4.7\" seatSeat Width 23.6\\'Height19.3\" -22.2\"Depth 32.5\\'Width 28.3\\''}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:39.942856Z",
     "iopub.status.busy": "2024-11-25T07:05:39.942515Z",
     "iopub.status.idle": "2024-11-25T07:05:39.948701Z",
     "shell.execute_reply": "2024-11-25T07:05:39.947598Z",
     "shell.execute_reply.started": "2024-11-25T07:05:39.942826Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_values(text):\n",
    "    try:\n",
    "        \n",
    "        if isinstance(text, dict):\n",
    "            return ' '.join(str(value) for value in text.values())\n",
    "        # If the input is a string, parse it into a dictionary\n",
    "        data_dict = ast.literal_eval(text)\n",
    "        if isinstance(data_dict, dict):\n",
    "            return ' '.join(str(value) for value in data_dict.values())\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Handle cases where parsing fails\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "text_data = extract_values(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:40.356318Z",
     "iopub.status.busy": "2024-11-25T07:05:40.355444Z",
     "iopub.status.idle": "2024-11-25T07:05:40.361316Z",
     "shell.execute_reply": "2024-11-25T07:05:40.360445Z",
     "shell.execute_reply.started": "2024-11-25T07:05:40.356286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'400 LBSpressure < 400 lbsBackrestArm Length 14.6\"Height29.9\"Seat Depth 19.9\".Height44.5\" -47.4\"4.7\" seatSeat Width 23.6\\'Height19.3\" -22.2\"Depth 32.5\\'Width 28.3\\''"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:41.355218Z",
     "iopub.status.busy": "2024-11-25T07:05:41.354297Z",
     "iopub.status.idle": "2024-11-25T07:05:41.359704Z",
     "shell.execute_reply": "2024-11-25T07:05:41.358564Z",
     "shell.execute_reply.started": "2024-11-25T07:05:41.355182Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "       \n",
    "        return re.sub(r'[^a-zA-Z0-9\\s.,!?;:()\\[\\]\\'\"@#$%&*+-/=_<>|]', '', text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:41.962258Z",
     "iopub.status.busy": "2024-11-25T07:05:41.961421Z",
     "iopub.status.idle": "2024-11-25T07:05:41.966276Z",
     "shell.execute_reply": "2024-11-25T07:05:41.965128Z",
     "shell.execute_reply.started": "2024-11-25T07:05:41.962223Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = clean_text(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:42.415388Z",
     "iopub.status.busy": "2024-11-25T07:05:42.414990Z",
     "iopub.status.idle": "2024-11-25T07:05:42.420086Z",
     "shell.execute_reply": "2024-11-25T07:05:42.419010Z",
     "shell.execute_reply.started": "2024-11-25T07:05:42.415351Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data  = text_data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:42.888486Z",
     "iopub.status.busy": "2024-11-25T07:05:42.887621Z",
     "iopub.status.idle": "2024-11-25T07:05:42.892444Z",
     "shell.execute_reply": "2024-11-25T07:05:42.891579Z",
     "shell.execute_reply.started": "2024-11-25T07:05:42.888450Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_space_before_numbers(text):\n",
    "\n",
    "    updated_text = re.sub(r'(?<![0-9\\s.])(\\d)', r' \\1', text)\n",
    "    return updated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:44.158727Z",
     "iopub.status.busy": "2024-11-25T07:05:44.158392Z",
     "iopub.status.idle": "2024-11-25T07:05:44.163160Z",
     "shell.execute_reply": "2024-11-25T07:05:44.162200Z",
     "shell.execute_reply.started": "2024-11-25T07:05:44.158700Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = add_space_before_numbers(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:44.863077Z",
     "iopub.status.busy": "2024-11-25T07:05:44.862694Z",
     "iopub.status.idle": "2024-11-25T07:05:44.869354Z",
     "shell.execute_reply": "2024-11-25T07:05:44.868458Z",
     "shell.execute_reply.started": "2024-11-25T07:05:44.863044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str): \n",
    "        return \" \".join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:45.833778Z",
     "iopub.status.busy": "2024-11-25T07:05:45.833423Z",
     "iopub.status.idle": "2024-11-25T07:05:45.838286Z",
     "shell.execute_reply": "2024-11-25T07:05:45.837178Z",
     "shell.execute_reply.started": "2024-11-25T07:05:45.833747Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = remove_stopwords(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:46.933280Z",
     "iopub.status.busy": "2024-11-25T07:05:46.932373Z",
     "iopub.status.idle": "2024-11-25T07:05:46.938501Z",
     "shell.execute_reply": "2024-11-25T07:05:46.937448Z",
     "shell.execute_reply.started": "2024-11-25T07:05:46.933245Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_weight_separation(input_text):\n",
    "\n",
    "    units = [\n",
    "        \"g\", \"kg\", \"mg\", \"ml\", \"l\", \"oz\", \"lb\", \"pcs\", \"ct\", \"cfqty\",  \n",
    "        \"gram\", \"kilogram\", \"microgram\", \"milligram\", \"litre\", \"liter\", \"ounce\", \"pound\", \"ton\", \"piece\", \"pieces\" \n",
    "    ]\n",
    "    \n",
    "\n",
    "    unit_pattern = \"|\".join(units) \n",
    "    pattern = rf\"(\\d+(\\.\\d+)?(?:{unit_pattern}))\"  \n",
    "    \n",
    "    corrected_text = re.sub(pattern, r\" \\1 \", input_text)\n",
    "\n",
    "    corrected_text = re.sub(r\"\\s{2,}\", \" \", corrected_text).strip()\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:47.627358Z",
     "iopub.status.busy": "2024-11-25T07:05:47.627005Z",
     "iopub.status.idle": "2024-11-25T07:05:47.631777Z",
     "shell.execute_reply": "2024-11-25T07:05:47.630824Z",
     "shell.execute_reply.started": "2024-11-25T07:05:47.627327Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = correct_weight_separation(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:48.317365Z",
     "iopub.status.busy": "2024-11-25T07:05:48.316665Z",
     "iopub.status.idle": "2024-11-25T07:05:48.322757Z",
     "shell.execute_reply": "2024-11-25T07:05:48.321820Z",
     "shell.execute_reply.started": "2024-11-25T07:05:48.317329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'400 lbspressure < 400 lbsbackrestarm length 14.6\"height 29.9\"seat depth 19.9\".height 44.5\" - 47.4\" 4.7\" seatseat width 23.6\\'height 19.3\" - 22.2\"depth 32.5\\'width 28.3\\''"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:50.108125Z",
     "iopub.status.busy": "2024-11-25T07:05:50.107650Z",
     "iopub.status.idle": "2024-11-25T07:05:50.113325Z",
     "shell.execute_reply": "2024-11-25T07:05:50.112289Z",
     "shell.execute_reply.started": "2024-11-25T07:05:50.108093Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_dimension_separation(input_text):\n",
    "\n",
    "    units = [\n",
    "        \"cm\", \"mm\", \"m\", \"in\", \"inch\", \"ft\", \"yard\", \n",
    "        \"centimetre\", \"millimetre\", \"metre\", \"foot\", \"yard\", \"inches\"  \n",
    "    ]   \n",
    "    unit_pattern = \"|\".join(units) \n",
    "    pattern = rf\"(\\d+(\\.\\d+)?(?:{unit_pattern}))\"\n",
    "\n",
    "    corrected_text = re.sub(pattern, r\" \\1 \", input_text)\n",
    "\n",
    "    corrected_text = re.sub(r\"\\s{2,}\", \" \", corrected_text).strip()\n",
    "    \n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:50.824998Z",
     "iopub.status.busy": "2024-11-25T07:05:50.824617Z",
     "iopub.status.idle": "2024-11-25T07:05:50.830296Z",
     "shell.execute_reply": "2024-11-25T07:05:50.829313Z",
     "shell.execute_reply.started": "2024-11-25T07:05:50.824965Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = correct_dimension_separation(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:51.881089Z",
     "iopub.status.busy": "2024-11-25T07:05:51.880720Z",
     "iopub.status.idle": "2024-11-25T07:05:51.888220Z",
     "shell.execute_reply": "2024-11-25T07:05:51.887236Z",
     "shell.execute_reply.started": "2024-11-25T07:05:51.881058Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_quantity_separation(input_text):\n",
    "\n",
    "    voltage_units = [\n",
    "        \"kV\", \"mV\", \"V\", \"kilovolt\", \"millivolt\", \"volt\" , \"v\" , \"kv\" , \"mv\"\n",
    "    ]\n",
    "    \n",
    "    wattage_units = [\n",
    "        \"kW\", \"W\", \"kilowatt\", \"watt\" , \"w\" , \"kw\"\n",
    "    ]\n",
    "    \n",
    "    volume_units = [\n",
    "        \"cl\", \"centilitre\", \"ft³\", \"cubic foot\", \"in³\", \"cubic inch\", \"cup\", \"dl\", \"decilitre\",\n",
    "        \"fl oz\", \"fluid ounce\", \"gal\", \"gallon\", \"imp gal\", \"imperial gallon\", \"l\", \"litre\", \"litres\"\n",
    "        \"µl\", \"microlitre\", \"ml\", \"millilitre\", \"pt\", \"pint\", \"qt\", \"quart\"\n",
    "    ]\n",
    "    \n",
    "    voltage_pattern = r\"(\\d+(\\.\\d+)?(?:{}))\".format(\"|\".join(voltage_units))\n",
    "    wattage_pattern = r\"(\\d+(\\.\\d+)?(?:{}))\".format(\"|\".join(wattage_units))\n",
    "    volume_pattern = r\"(\\d+(\\.\\d+)?(?:{}))\".format(\"|\".join(volume_units))\n",
    "\n",
    "    def add_space_after_unit(pattern, text):\n",
    "        return re.sub(pattern, r\" \\1 \", text)\n",
    "    \n",
    "    corrected_text = input_text\n",
    "    corrected_text = add_space_after_unit(voltage_pattern, corrected_text)\n",
    "    corrected_text = add_space_after_unit(wattage_pattern, corrected_text)\n",
    "    corrected_text = add_space_after_unit(volume_pattern, corrected_text)\n",
    "    \n",
    "    corrected_text = re.sub(r\"\\s{2,}\", \" \", corrected_text).strip()\n",
    "    \n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:52.811572Z",
     "iopub.status.busy": "2024-11-25T07:05:52.811213Z",
     "iopub.status.idle": "2024-11-25T07:05:52.815927Z",
     "shell.execute_reply": "2024-11-25T07:05:52.814952Z",
     "shell.execute_reply.started": "2024-11-25T07:05:52.811540Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = correct_quantity_separation(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:05:53.861729Z",
     "iopub.status.busy": "2024-11-25T07:05:53.861378Z",
     "iopub.status.idle": "2024-11-25T07:05:53.867870Z",
     "shell.execute_reply": "2024-11-25T07:05:53.866865Z",
     "shell.execute_reply.started": "2024-11-25T07:05:53.861698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'400 lbspressure < 400 lbsbackrestarm length 14.6\"height 29.9\"seat depth 19.9\".height 44.5\" - 47.4\" 4.7\" seatseat width 23.6\\'height 19.3\" - 22.2\"depth 32.5\\'width 28.3\\''"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:48:33.969238Z",
     "iopub.status.busy": "2024-11-25T06:48:33.968313Z",
     "iopub.status.idle": "2024-11-25T06:48:35.441980Z",
     "shell.execute_reply": "2024-11-25T06:48:35.441137Z",
     "shell.execute_reply.started": "2024-11-25T06:48:33.969197Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:48:35.443920Z",
     "iopub.status.busy": "2024-11-25T06:48:35.443328Z",
     "iopub.status.idle": "2024-11-25T06:48:36.426179Z",
     "shell.execute_reply": "2024-11-25T06:48:36.425262Z",
     "shell.execute_reply.started": "2024-11-25T06:48:35.443872Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "model_dir = \"/kaggle/input/btp_t5_4epoch/transformers/default/1\"\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:48:36.996809Z",
     "iopub.status.busy": "2024-11-25T06:48:36.995732Z",
     "iopub.status.idle": "2024-11-25T06:48:37.631535Z",
     "shell.execute_reply": "2024-11-25T06:48:37.630474Z",
     "shell.execute_reply.started": "2024-11-25T06:48:36.996770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3168c6586d6b4c008c9891716bacfa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d6e83e8011411fb1bbab56c7780419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf517531a44a42978ea7d05aa8f4f0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:06:55.149090Z",
     "iopub.status.busy": "2024-11-25T07:06:55.148209Z",
     "iopub.status.idle": "2024-11-25T07:06:55.246825Z",
     "shell.execute_reply": "2024-11-25T07:06:55.245911Z",
     "shell.execute_reply.started": "2024-11-25T07:06:55.149051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted entity value: 400 pound\n"
     ]
    }
   ],
   "source": [
    "def extract_entity_value(group_id, entity_name, text):\n",
    "    input_text = f\"Group ID: {group_id}, Entity Name: {entity_name}, Text: {text}\"\n",
    "    inputs = tokenizer(f\"Extract {entity_name} from: {input_text}\", return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    device = next(model1.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model1.generate(**inputs, max_length=32, num_return_sequences=1, do_sample=False)\n",
    "    \n",
    "    outputs = outputs.cpu()\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "model1.to('cuda')\n",
    "\n",
    "test_group_id = 801829\n",
    "test_entity_name = \"maximum_weight_recommendation\"\n",
    "test_text = text_data\n",
    "predicted_value = extract_entity_value(test_group_id, test_entity_name, test_text)\n",
    "print(f\"Predicted entity value: {predicted_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T06:48:50.671588Z",
     "iopub.status.busy": "2024-11-25T06:48:50.670894Z",
     "iopub.status.idle": "2024-11-25T06:49:47.704806Z",
     "shell.execute_reply": "2024-11-25T06:49:47.703940Z",
     "shell.execute_reply.started": "2024-11-25T06:48:50.671553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2637ad6fa5564004a75a3106504d0d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddac929c075c4a83a33221ffc19b9df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_multitask_performance.png:   0%|          | 0.00/76.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60c10636e304f009c65a7d64161b381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0274683b8f40b3a87a2644a2790fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/880 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba2a15d1f104630a0734dc9047fb3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763bbebf51854a95b10390bf3a7f9ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3487f26b47514d979091c6e2de687dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d6bc9d9e47493f9b09c0fade5a2247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4b70455b024871ad98e3dd6793faf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model2 = GLiNER.from_pretrained(\"knowledgator/gliner-multitask-large-v0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:07:19.596194Z",
     "iopub.status.busy": "2024-11-25T07:07:19.595802Z",
     "iopub.status.idle": "2024-11-25T07:07:20.489621Z",
     "shell.execute_reply": "2024-11-25T07:07:20.488588Z",
     "shell.execute_reply.started": "2024-11-25T07:07:19.596156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.9\" => height\n",
      "19.9\" => depth\n",
      "44.5\" - 47.4\" => height\n",
      "23.6' => width\n",
      "19.3\" - 22.2\" => height\n",
      "32.5' => depth\n",
      "28.3' => width\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "labels = ['item_weight', 'item_volume', 'voltage', 'wattage',\n",
    "       'maximum_weight_recommendation', 'height', 'depth', 'width']\n",
    "\n",
    "entities = model2.predict_entities(text_data, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 173630,
     "modelInstanceId": 151157,
     "sourceId": 177436,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
